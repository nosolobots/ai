{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12909071",
   "metadata": {},
   "source": [
    "## La Función V y la ecuación de Bellman\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6336e396",
   "metadata": {},
   "source": [
    "### 1. Funciones de Valor\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f59ff3a",
   "metadata": {},
   "source": [
    "- #### Función V\n",
    "\n",
    "La **función de Valor** (_Value function_), **V(s)**, representa **cómo de bueno** es para el agente encontrarse en un determinado estado. Es igual al **retorno** (recompensa total futura) empezando desde dicho estado _s_.\n",
    "\n",
    "Dado que el retorno puede ser distinto en cada episodio, empleamos la **esperanza matemática** del retorno con descuento promediando múltiples episodios (sumatorio de la probabilidad de cada suceso aleatorio por el valor de dicho suceso).\n",
    "\n",
    "Para un entorno estocástico, tendremos que:\n",
    "\n",
    "<br><img src=\"funcion_v.png\"/><br>\n",
    "\n",
    "- #### Función Q\n",
    "\n",
    "La **función de Valor de la Acción** (_Q function_), **Q(s,a)**, representa cómo de bueno es para el agente, que se encuentra en un estado concreto, es realizar una determinada acción en base a una política dada (π). Es igual al retorno (recompensa total futura) al realizar la acción **a**, empezando en el estado **s** en el _timestep_ **t** y siguiendo la política **π**.\n",
    "\n",
    "<br><img src=\"funcion_q.png\"/><br>\n",
    "\n",
    "- #### Relación V/Q\n",
    "\n",
    "Denotamos como **π(a|s)** la probabilidad de que una politíca **π** estocástica seleccione la acción _a_ cuando se encuentra en el estado _s_, cumpliéndose que la suma de las probabilidades de todas las acciones salientes de un estado dado siempre es igual a 1.\n",
    "\n",
    "Así, podemos afirmar que la función de valor del estado es quivalente a la suma de las funciones de de valor de acción de todas las acciones _a_ salientes (de un estado _s_), multiplicada por la probabilidad de seleccionar cada acción respectica de acuerdo con la politica estocástica que guía al agente:\n",
    "\n",
    "<br><img src=\"relacion_vq.png\"/><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a045f0df",
   "metadata": {},
   "source": [
    "### 2. La ecuación de Bellman\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a323af84",
   "metadata": {},
   "source": [
    "- #### La ecuación de Bellman para la función V\n",
    "\n",
    "Recordemos que el **retorno con descuento** en el _time step t_ viene dado por el sumatorio (con descuento) de las sucesivas recompensas hasta el final del episodio, podemos escribirlo como la siguiente relación recursiva:\n",
    "\n",
    "<br><img src=\"bellman_G.png\"/><br>\n",
    "\n",
    "Del mismo modo, podemos escribir la **ecuación de Bellman para la función V** (para un entorno determinista) como:\n",
    "\n",
    "<br><img src=\"bellman_V.png\"/><br>\n",
    "\n",
    "- #### La ecuación de Bellman para la función V en entornos estocásticos\n",
    "\n",
    "En caso de hallarnos en un entorno estocástico, donde no podemos garantizar que, dada una acción _a_ en un estado _s_ acabemos siempre en el estado _s'_, modificaremos la ecuación anterior para incluir la **probabilidad P** de alcanzar el estado _s'_ desde el estado _s_ realizando la acción _a_. Es decir, estamos calculando la esperanza matemática a partir de cada potencial siguiente transición de estado.\n",
    "\n",
    "<br><img src=\"bellman_V_ee.png\"/><br>\n",
    "\n",
    "- #### La ecuación de Bellman para la función V para políticas estocásticas\n",
    "\n",
    "La ecuación anterior sería válida para entornos estocásticos pero políticas no estocásticas. En caso de una política estocástica, dado un estado _s_, seleccionamos una acción en base a cierta distribución de probabilidades.\n",
    "\n",
    "Para incluir la naturaleza estocástica de la política en la ecuación de Bellman, podemos utilizar nuevamente la esperanza matemática, añadiendo a la esxpresión anterior la probabilidad de la acción correspondiente:\n",
    "\n",
    "<br><img src=\"bellman_V_pe.png\"/><br>\n",
    "\n",
    "La ecuación anterior, también conocida como **_Bellman expectation equation_ de la función V**, que considera la estocasticidad tanto del entorno como de la política, suele escribirse como:\n",
    "\n",
    "<br><img src=\"bellman_V_pe2.png\"/><br>\n",
    "\n",
    "- #### La ecuación de Bellman para la función Q\n",
    "\n",
    "Del mismo modo, podemos definir las ecuaciones de Bellman para la **función Q** tanto para entornos deterministas como para entornos estocásticos y con políticas estocásticas:\n",
    "\n",
    "<br><img src=\"bellman_Q.png\"/><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba741c2",
   "metadata": {},
   "source": [
    "### 3. La ecuación de Bellman óptima\n",
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9c144fcc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "226a2562",
   "metadata": {},
   "source": [
    "- #### La Función V óptima\n",
    "\n",
    "De las ecuaciones anteriores, observamos que el valor de un estado dependerá de la política seguida. Definiremos por tanto la **función V óptima V<sub>*</sub>(s)** como aquella que produce el valor máximo en comparación con todas las demás funciones de valor:\n",
    "\n",
    "<br><img src=\"V_optima.png\"/><br>\n",
    "\n",
    "Podemos calcular la **ecuación de Bellman óptima para la función V** seleccionando aquella acción (de entre las posibles para el estado) que nos devuelve el valor máximo. Dado que no estamos utilizando ninguna política, podemos eliminar la esperanza matemática sobre la política π, resultando en la siguiente expresión: \n",
    "\n",
    "<br><img src=\"bellman_V_optima.png\"/><br>\n",
    "\n",
    "- #### La Función Q óptima\n",
    "\n",
    "Del mismo modo, tendremos que la **función Q óptima Q<sub>*</sub>(s,a)** vendrá dada por:\n",
    "\n",
    "<br><img src=\"Q_optima.png\"/><br>\n",
    "\n",
    "Para calcular la **ecuación de Bellman óptima para la función Q**, en lugar de usar la política para seleccionar la acción _a'_ en el siguiente estado _s'_, elegimos todas las acciones posibles y calculamos el valor máximo de Q(s',a'):\n",
    "\n",
    "<br><img src=\"bellman_Q_optima.png\"/><br>\n",
    "\n",
    "- #### Relación V<sub>*</sub>/Q<sub>*</sub>\n",
    "\n",
    "Finalmente, nos queda añadir el hecho de que el valor óptimo de un estado, V<sub>*</sub>(s), es igual a la mejor función de valor de acción que podamos obtener a partir de ese estado, es decir:\n",
    "\n",
    "<br><img src=\"relacion_vq_optima.png\"/><br>\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
