{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes neuronales I. Fundamentos\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Neuronas artificiales\n",
    "---\n",
    "La base del aprendizaje profundo (_deep learning_) son las neuronas artificiales: entidades matemáticas capaces de representar complicadas funciones mediante la composición de funciones simples.\n",
    "\n",
    "Aunque los modelos iniciales se inspiraron en la neurociencia (_perceptron_), las ANN modernas tienen sólo un ligero parecido con los mecanismos de las neronas biológicas. En todo caso, parece que las redes neuronales biológics emplean estrategisa matemáticas similares para la aproximación de funciones complicadas debido a sus eficacia.\n",
    "\n",
    "El bloque constructivo fundamental de estas complicadas funciones es la **neurona**. En realidad, no es más que una **transformación lineal** de la entrada (por ejemplo, multiplicando la entrada por un número o _weight_ y añadiendo una constante o _bias_) seguida por la aplicación de una **función no-lineal** ó **activación**. Matemáticamente, podemos escribirlo como: **o = f(w\\*x + b)**, donde _(w\\*x + b)_ es la transformación lineal y _f_ representa la función de activación.\n",
    "\n",
    "<br>\n",
    "\n",
    "![data/image-lect/a_neuron.png](data/image-lect/a_neuron.png)\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Componiendo una red multi-capa\n",
    "---\n",
    "Una red neuronal multicapa no es más que la composición de funciones como la anterior:\n",
    "\n",
    "<br>\n",
    "\n",
    "`x_1 = f(w_0 * x + b_0)`<br>\n",
    "`x_2 = f(w_1 * x_1 + b_1)`<br>\n",
    "...<br>\n",
    "`y = f(w_n * x_n + b_n)`\n",
    "\n",
    "<br>\n",
    "\n",
    "donde la salida de una capa de neuronas es la entrada de la siguiente capa\n",
    "\n",
    "<br>\n",
    "\n",
    "![data/image-lect/ann.png](data/image-lect/ann.png)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 La función de _error_\n",
    "---\n",
    "Una diferencia importante entre nuestro modelo lineal precedente y el modelo usado en DL es la _forma_ de la función de error. Nuestro modelo lineal y la función de pérdida (_loss_) MSE, tenían una curva de error convexa que definía claramente un mínimo. Así, nosotros podemos actualizar nuestros parámetros en un intento de _estimar_ los valores que _minimicen_ el error y alcancen ese mínimo.\n",
    "\n",
    "Las redes neuronales no gozan de la misma propiedad de una superficie de error convexa. **No existe una única respuesta correcta para cada uno de los parámetros que estamos intentando aproximar**. Más bien, estamos buscando una **combinación de los parámetros** que produzcan los resultados deseados.\n",
    "\n",
    "Una de las razones por las que las redes neuronales no tienen superficies de error convexas es debido a la **función de activación**. Como contrapartida, la fantástica capacidad de las neuronas de aproximar tan amplio rango de funciones depende de la combinación del comportamiento lineal y no-lineal inherente a cada neurona."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 La función de _activación_\n",
    "---\n",
    "Como acabamos de describir, la unidad mínima de las redes neuonales profundas, la neurona, es una combinación de una operación lineal seguida por una función de activación. Ésta última realiza dos acciones importantes:\n",
    "\n",
    "- En las partes más internas (profundas) del modelo, permite a la función de salida tener diferentes pendientes en diferentes valores. Esto le permitirá a la red aproximar funciones arbitrarias por **composición** de funciones\n",
    "\n",
    "- En la última de las capas, juega el papel de _concentrar_ las salidas de las operaciones lineales precedentes en un rango determinado\n",
    "\n",
    "En la siguiente imagen podemos apreciar el efecto de esto último, donde representamos la salida de un modelo de clasificación de imágenes de _dogo_\n",
    "\n",
    "<br>\n",
    "\n",
    "![data/image-lect/dog_activation.png](data/image-lect/dog_activation.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "Estas funciones de activación no-lineal (_sigmoid_, _tanh_, _relu_,...) nos permiten:\n",
    "\n",
    "- Capar la salida en un rango apropiado (0, 1), (-1-1),...\n",
    "- Presentan una zona de transición central que le permite a la neurona ajustar de forma fina la salida en base a los parámetros (pequeños cambios en éstos, tienen su reflejo en la salida resultante). En el ejemplo anterior, dos tipos de oso como el _grizzly_ y el _polar_ (que tiene una cara ligeramente más parecida a la de un perro), serían \"distinguidos\" por la neurona y recibirían puntuaciones diferentes sobre el eje Y, estando el _polar_ \"ligeramente\" más próximo al 1.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
