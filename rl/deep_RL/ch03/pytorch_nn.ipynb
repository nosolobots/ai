{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Neural Networks\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El módulo **torch.nn** nos proporciona diferentes clases que actúan como bloques básicos de funcionalidad. Todos soportan _minibatch_, tienen valores por defecto apropiados y pesos inicializados adecuadamente.\n",
    "\n",
    "Por ejemplo, la clase **Linear** implementa una capa (_layer_) **_feed-forward_** con _bias_ opcional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9499, -0.5711,  0.4097, -0.2375, -0.7877], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "OrderedDict([('weight', tensor([[-0.6053, -0.4301],\n",
      "        [ 0.1808, -0.3013],\n",
      "        [ 0.5126, -0.3676],\n",
      "        [-0.5687, -0.0257],\n",
      "        [ 0.6280, -0.5597]], device='cuda:0')), ('bias', tensor([ 0.5156, -0.1493,  0.6324,  0.3825, -0.2964], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "l = nn.Linear(2, 5).to('cuda:0')\n",
    "v = torch.cuda.FloatTensor([1, 2])\n",
    "\n",
    "print(l(v))\n",
    "print(l.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el ejemplo anterior creamos un _layer_ con 2 _input units_ y 5 _output units_ incializado de forma aleatoria\n",
    "<br><br>\n",
    "Utilizando el \"bloque\" anterior, podemos crear una red neuronal más compleja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0853, 0.1531, 0.0853, 0.1108, 0.0944, 0.1275, 0.1368, 0.0853, 0.0732,\n",
      "         0.0484]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "OrderedDict([('0.weight', tensor([[ 0.1100,  0.1216],\n",
      "        [ 0.3805,  0.3622],\n",
      "        [-0.1442, -0.0911],\n",
      "        [-0.1536,  0.6588],\n",
      "        [ 0.2124, -0.4037]], device='cuda:0')), ('0.bias', tensor([-0.2994, -0.3896,  0.4930,  0.5828, -0.1186], device='cuda:0')), ('2.weight', tensor([[ 0.3696,  0.0796,  0.2030,  0.3606,  0.2135],\n",
      "        [-0.1396, -0.0770,  0.0380, -0.4046,  0.1174],\n",
      "        [-0.3779, -0.4092, -0.0545,  0.4411, -0.3374],\n",
      "        [ 0.0137, -0.2603, -0.4339,  0.3547,  0.1449],\n",
      "        [-0.1976,  0.1127,  0.1789,  0.4126,  0.0670],\n",
      "        [ 0.3309, -0.3578,  0.4132,  0.2615, -0.2890],\n",
      "        [-0.3347, -0.0328,  0.3711,  0.0957,  0.2138],\n",
      "        [ 0.3658, -0.3052,  0.1057, -0.1192, -0.1160],\n",
      "        [ 0.1716,  0.3976,  0.3393,  0.3232, -0.3418],\n",
      "        [ 0.1924,  0.2277, -0.2342, -0.2471,  0.3659],\n",
      "        [-0.2504,  0.2720,  0.2795, -0.3585, -0.1496],\n",
      "        [-0.3348,  0.2654,  0.2966, -0.4055, -0.4143],\n",
      "        [ 0.2267,  0.2313, -0.3993, -0.0226, -0.3881],\n",
      "        [ 0.3692,  0.4325, -0.2589, -0.2626, -0.0877],\n",
      "        [ 0.3542,  0.1050,  0.3992, -0.2518,  0.4352],\n",
      "        [-0.2105,  0.0251, -0.0126, -0.2416,  0.0753],\n",
      "        [-0.2144, -0.0963,  0.2400, -0.2997,  0.2282],\n",
      "        [-0.2797, -0.1433,  0.3407, -0.0993,  0.1308],\n",
      "        [-0.2262,  0.3295,  0.1712, -0.1303, -0.3452],\n",
      "        [ 0.1422,  0.3078, -0.1341,  0.2810,  0.0969]], device='cuda:0')), ('2.bias', tensor([-0.3104, -0.2258, -0.0171, -0.3601, -0.0614,  0.1098,  0.2242, -0.0446,\n",
      "        -0.3323, -0.0073, -0.2612, -0.2827,  0.2557,  0.0665,  0.2535,  0.1386,\n",
      "        -0.2083,  0.3852, -0.1381, -0.0868], device='cuda:0')), ('4.weight', tensor([[-0.1861, -0.0083, -0.0271, -0.0087, -0.0803,  0.1519,  0.0959,  0.0685,\n",
      "         -0.1132, -0.0432,  0.0679, -0.0280, -0.1450, -0.1999, -0.1896, -0.2156,\n",
      "         -0.1568,  0.0338,  0.0741,  0.1026],\n",
      "        [ 0.1189, -0.0943,  0.0096,  0.1337,  0.0725,  0.1820, -0.1057,  0.0975,\n",
      "          0.1009, -0.1791, -0.0123, -0.1178,  0.0126, -0.1780, -0.1042,  0.0888,\n",
      "         -0.1873, -0.1217, -0.2098,  0.2129],\n",
      "        [ 0.1400, -0.0825, -0.0681,  0.0414,  0.1231,  0.0394,  0.1413,  0.0454,\n",
      "          0.0138,  0.1467, -0.0662, -0.1297,  0.0560,  0.1304, -0.1628, -0.1383,\n",
      "          0.0704, -0.0945, -0.0640,  0.1084],\n",
      "        [-0.1301,  0.0470,  0.0814,  0.2102, -0.0128, -0.1900, -0.1289,  0.1518,\n",
      "          0.2207,  0.0495, -0.0926,  0.0407,  0.2053,  0.1404,  0.0353, -0.0245,\n",
      "         -0.0116, -0.0451, -0.0900,  0.1541],\n",
      "        [ 0.0467, -0.1408, -0.0415,  0.0323, -0.1100,  0.1056,  0.1009,  0.0499,\n",
      "          0.1083, -0.1355,  0.0346,  0.0692, -0.1093, -0.2233, -0.2152, -0.0288,\n",
      "         -0.0275, -0.1837, -0.1236, -0.1500],\n",
      "        [ 0.0351, -0.1348, -0.0072,  0.1801,  0.0212,  0.1950, -0.1508,  0.0361,\n",
      "          0.1786, -0.2093, -0.0036, -0.1053,  0.1457, -0.0552,  0.0577, -0.0712,\n",
      "         -0.0473, -0.0421, -0.0448,  0.0734],\n",
      "        [-0.1057, -0.0567, -0.0064, -0.1316,  0.1181,  0.1770,  0.1838, -0.1643,\n",
      "          0.2074, -0.0302,  0.1871, -0.1344,  0.1094, -0.0006, -0.0549, -0.1334,\n",
      "         -0.0963, -0.2219, -0.0625, -0.0596],\n",
      "        [-0.1287, -0.0416,  0.1154,  0.0899,  0.1014,  0.1978,  0.0116,  0.1921,\n",
      "         -0.0918,  0.0319,  0.0480, -0.0169,  0.0182,  0.0306, -0.0377, -0.0392,\n",
      "          0.0151,  0.0360,  0.1077,  0.1965],\n",
      "        [-0.1901, -0.1312,  0.0420,  0.0397,  0.1176,  0.0057, -0.1922,  0.0149,\n",
      "         -0.1360, -0.0503,  0.1505, -0.1341, -0.2183,  0.1453,  0.0113,  0.1962,\n",
      "         -0.2207,  0.0976, -0.0567, -0.1146],\n",
      "        [ 0.0826, -0.0051, -0.2217,  0.1307, -0.0005, -0.1761, -0.1009,  0.0200,\n",
      "          0.1184, -0.1680,  0.0766,  0.1436, -0.1125,  0.0197,  0.1913,  0.1825,\n",
      "         -0.0794, -0.1216, -0.1064, -0.1659]], device='cuda:0')), ('4.bias', tensor([-0.2089,  0.0958, -0.1948,  0.0593,  0.1604,  0.0478,  0.0567,  0.1553,\n",
      "         0.1497, -0.1378], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "s = nn.Sequential(\n",
    "    nn.Linear(2, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10),\n",
    "    nn.Dropout(p=0.3),\n",
    "    nn.Softmax(dim=1)\n",
    ").to('cuda:0')\n",
    "\n",
    "print(s(torch.cuda.FloatTensor([[1, 2]])))\n",
    "print(s.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Custom layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos crearnos nuestras propias \"personalizaciones\" de estos bloques funcionales. Para ello extenderemos la clase **nn.Module** y sobreescribiremos su método **forward()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurModule(nn.Module):\n",
    "    def __init__(self, num_inputs, num_classes, dropout_prob=0.3):\n",
    "        super().__init__()\n",
    "        self.pipe = nn.Sequential(\n",
    "            nn.Linear(num_inputs, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, num_classes),\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.pipe(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OurModule(\n",
      "  (pipe): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=5, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=5, out_features=20, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Softmax(dim=1)\n",
      "  )\n",
      ")\n",
      "tensor([[0.2312, 0.3117, 0.4571]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "net = OurModule(num_inputs=2, num_classes=3)\n",
    "net = net.to('cuda:0')\n",
    "v = torch.cuda.FloatTensor([[2, 3]])\n",
    "out = net(v)\n",
    "print(net)\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
