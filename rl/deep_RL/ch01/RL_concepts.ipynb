{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical foundations of RL\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov decision processes (MDP)\n",
    "Markov process (MP) + rewards --> Markov reward process\n",
    "\n",
    "Markov reward process + actions --> Markov decision process (MDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Markov process\n",
    "Imagine that you have some system in front of you that you can **only** observe. What you observe is called **states**, and the system can switch beteween states according to some laws of dynamics.\n",
    "\n",
    "All posible states are called the **state space**. For MPs, we require this set of states to be **finite**. Our observations form a sequence of states or a **chain** (Markov chain)\n",
    "\n",
    "For example, looking at the simplest model fo the weather in some city, we can observe the current day as _sunny_ or _cloudy_, which is our state space. A sequence of observations over time forms a chain of states, such as {_sunny ‚Üí sunny ‚Üí rainy ‚Üí sunny..._}, and this is called **episode**\n",
    "\n",
    "To call such a system a MP, it needs to fulfill the **Markov property**, which means that the future system dynamics from any state have to depend on this state **only**. In other words, the Markov property makes every observable state self-contained to describe the **future** of the system. **Only one state is required to model the future dynamics of the system** and not the whole history or, say, the last _N_ states.\n",
    "\n",
    "If we want to make our model more complex, we can do this by extending our state space. For example, adding summer and winter seasons so our new state space would be {_sunny+summer, sunny+winter, rainy+summer, rainy+winter_}.\n",
    "\n",
    "As our system model complies with the Markov property, you can capture transition probabilities with a **transition matrix**, which is a square matrix of the size _NxN_, where _N_ is the number of states in our model. For example,\n",
    "\n",
    "<br>\n",
    "\n",
    "| | Sunny | Rainy |\n",
    "| --- | --- | --- |\n",
    "| Sunny | 0.8 | 0.2 |\n",
    "| Rainy | 0.1 | 0.9 |\n",
    "\n",
    "<br>\n",
    "\n",
    "The **formal definition of an MP** is as follows:\n",
    "* A **set of states (_S_)** that a system can be in\n",
    "* A **transition matrix (_T_)**, with transition probabilities, which defines the system dynamics.\n",
    "\n",
    "<br>\n",
    "\n",
    "A useful visual representation of an MP is a **graph** whith nodes corresponding to system states and edges, labeled with probabilities, representing a possible transition from state to state.\n",
    "\n",
    "<br>\n",
    "\n",
    "![img/weather_mp.png](img/weather_mp.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "It's important to understand the difference between the actual transitions observed in an episode and the underlying distribution given in the transition matrix. Concrete episodes that we observe are randomly sampled from the distribution of the model, so they can _differ_ from episode to episode. However, the probability of the concrete transition to be sampled remains the same.\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Markov reward processes\n",
    "To introduce reward, we need to extend our MP model. We need to add **value** to our transition from state to state. In general, we'll have another square matrix, similar to the transition matrix, with reward given for transitioning from state i to state j. This reward could be positive or negative, large or small. In some cases, the reward will depend only for reaching the state, regardless of the previous state.\n",
    "\n",
    "The second thing we need to add to the model is the **discount factor** ùõæ (_gamma_), which is a single number in the range \\[0, 1\\]\n",
    "\n",
    "With these two new additions to our model, we can define the **return at time _t_** of an episode as:\n",
    "\n",
    "![img/f_value_of_state.png](img/f_value_of_state.png)\n",
    "\n",
    "\n",
    "For every time point, we calculate return as a **sum of subsequent rewards**, but with less weight of future rewards as we move forward in time.\n",
    "\n",
    "The discount factor stands for the **foresightened** of the agent. If gamma equals 1, then return G<sub>t</sub> just equals a sum of all subsequents rewards and corresponds to he agent that has perfect visibility of any subsequent rewards. If gamma equals 0, G<sub>t</sub> will be just immediate reward whitout any subsequent state and will correspond to absolute short-sightedness. The closer it is to 1, the more steps ahead of us we will take into account.\n",
    "\n",
    "This return quantity is not very useful in practice, as it was defined for every specific chain. A more valuable quantity is the **value of the state**, which is an average of the return over large number of chains, or the **expectation of return**:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Let's see an example\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### example: The Office Worker (Markov reward process)\n",
    "Our state space will be:\n",
    "* Home\n",
    "* Computer\n",
    "* Coffe\n",
    "* Chatting\n",
    "\n",
    "being the transition graph something like that\n",
    "\n",
    "<br>\n",
    "\n",
    "![img/states_graph.png](img/states_graph.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "We assume that our office worker's weekday starts from the **Home** state and that he starts his day with **Coffe** whitout exception. The diagram also shows that workdays always end going to the **Home** state from the **Computer** state.\n",
    "\n",
    "Our transition matrix (probabilities) could be something like:\n",
    "\n",
    "<br>\n",
    "\n",
    "| | Home | Coffee | Chat | Computer |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Home | 0.6 | 0.4 | 0.0 | 0.0 | \n",
    "| Coffee | 0.0 | 10.0 | 0.7 | 0.2 |\n",
    "| Chat | 0.0 | 0.2 | 0.5 | 0.3 |\n",
    "| Computer | 0.2 | 0.2 | 0.1 | 0.5 |\n",
    "\n",
    "<br>\n",
    "\n",
    "Resulting in the next transition graph:\n",
    "\n",
    "<br>\n",
    "\n",
    "![img/states_graph_with_prob.png](img/states_graph_with_prob.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's extend now our model with the following rewards and turn it into a **Office Worker reward process (RP)**\n",
    "\n",
    "* Home ‚Üí Home: 1\n",
    "* Home ‚Üí Coffe: 1\n",
    "* Computer ‚Üí Computer: 5\n",
    "* Computer ‚Üí Chat: -3\n",
    "* Chat ‚Üí Computer: 2\n",
    "* Computer ‚Üí Coffee: 1\n",
    "* Coffee ‚Üí Computer: 3\n",
    "* Coffee ‚Üí Coffee: 1\n",
    "* Coffee ‚Üí Chat: 2\n",
    "* Chat ‚Üí Coffee: 1\n",
    "* Chat ‚Üí Chat: -1\n",
    "\n",
    "The resulting diagram will be:\n",
    "\n",
    "<br>\n",
    "\n",
    "![img/states_graph_with_prob_and_reward.png](img/states_graph_with_prob_and_reward.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "Given the previous graph, how we could compute the **value of a state** (or **how good is to be in a state**)?\n",
    "\n",
    "For a given ùõæ = 0, our return (total future reward) is equeal only to the reward of the transition to the next state, multiply by the probability of transition:\n",
    "\n",
    "<br>\n",
    "\n",
    "* V(_chat_) = -1\\*0.5 + 2\\*0.3 + 1\\*0.2 = 0.3\n",
    "* V(_coffee_) = 2\\*0.7 + 1\\*0.1 + 3\\*0.2 = 2.1\n",
    "* V(_home_) = 1\\*0.6 + 1\\*0.4 = 1.0\n",
    "* V(_computer_) = 5\\*0.5 + (-3)\\*0.1 + 1\\*0.2 + 2\\*0.2 = 2.8\n",
    "\n",
    "<br>\n",
    "\n",
    "So, **Computer** is the most valuable state to be in (if we care only about immediate reward)\n",
    "\n",
    "<br>\n",
    "\n",
    "For a given ùõæ = 1, the value will be **infinite** for all states, as our model doesn't contain _sink_ states (states without outgoing transitions), so we could have potentially infinite number of transitions in the future (that is the reason whe have a ùõæ factor in the range \\[0, 1\\]). This ùõæ value is only useful if we're dealing with finite-horizon enviroments. For example, the tic-tac-toe game (episodes until nine steps) or multi-armed bandit (episodes of one step)\n",
    "\n",
    "<br>\n",
    "\n",
    "For ùõæ values greater than 0 and less than 1, by hand computation becomes almost impossible, so we need computers and clever methods (tabular learning, Bellman equation, q-learning,..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding actions. Markov decission processes (MDP)\n",
    "The next logic step would be to include actions in our model. First, we have to define a **finite** set of actions (_A_), our agent's **action space**. Secondly, we will extend our transtion matrix with a new dimension. Let's see how.\n",
    "\n",
    "Our previous transition matrix had a square form, with the source state in rows and target state in columns. So, for given state, we had the probabilities to jump to every state.\n",
    "\n",
    "<br>\n",
    "\n",
    "![img/transition_matrix.png](img/transition_matrix.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "As those jumps are consequence of an agent's action, we have to add a **depth dimension** with the probability of jump to the target state from the source state for every possible action.\n",
    "\n",
    "<br>\n",
    "\n",
    "![img/MDP_transition_matrix.png](img/MDP_transition_matrix.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "For example, let's imagine a small robot that lives in a 3x3 grid and can execute the actions _turn left, turn right_ and _go forward_. The state of the world is the robot position plus orientation (3 x 3 x 4 = 36 states).\n",
    "\n",
    "Also, consider a stochastic environment (non-deterministic) where for every action exists some probability such that action not being taken by the robot and stays in the same position.\n",
    "\n",
    "To capture all these details about the environment and possible reactions to the agent actions, the general MDP has a 2D transition matrix with the dimensions source state, action, and target state.\n",
    "\n",
    "<br>\n",
    "\n",
    "![img/gridworld.png](img/gridworld.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "Finally, to turn our Markov reward process into MDP, we need to add actions to our reward matrix in the same way that we did with the transition matrix. Our reward matrix will depend not only on the state but also on the action that leads to this state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy\n",
    "We can define **policy** as some set of rules that controls the agent's behavior.\n",
    "\n",
    "Given an environment, we can define different policies for our agents, which will lead to different sets of valid states. So, different policies will give them different amounts of return.\n",
    "\n",
    "Formally, policy is defined as the probabilty distribution over actions for every possible state (what is the probability of take an specific action given the actual state?)\n",
    "\n",
    "<br>\n",
    "\n",
    "![img/f_policy.png](img/f_policy.png)\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
