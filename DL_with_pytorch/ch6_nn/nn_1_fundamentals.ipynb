{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes neuronales I. Fundamentos\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Neuronas artificiales\n",
    "---\n",
    "La base del aprendizaje profundo (_deep learning_) son las neuronas artificiales: entidades matemáticas capaces de representar complicadas funciones mediante la composición de funciones simples.\n",
    "\n",
    "Aunque los modelos iniciales se inspiraron en la neurociencia (_perceptron_), las ANN modernas tienen sólo un ligero parecido con los mecanismos de las neronas biológicas. En todo caso, parece que las redes neuronales biológicas emplean estrategias matemáticas similares para la aproximación de funciones complicadas debido a sus eficacia.\n",
    "\n",
    "El bloque constructivo fundamental de estas complicadas funciones es la **neurona**. En realidad, no es más que una **transformación lineal** de la entrada (por ejemplo, multiplicando la entrada por un número o _weight_ y añadiendo una constante o _bias_) seguida por la aplicación de una función de **activación** no-lineal. Matemáticamente, podemos escribirlo como: **o = f(w\\*x + b)**, donde _(w\\*x + b)_ es la transformación lineal y _f_ representa la función de activación.\n",
    "\n",
    "<br>\n",
    "\n",
    "![data/image-lect/a_neuron.png](data/image-lect/a_neuron.png)\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Componiendo una red multi-capa\n",
    "---\n",
    "Una red neuronal multicapa no es más que la composición de funciones como la anterior:\n",
    "\n",
    "<br>\n",
    "\n",
    "`x_1 = f(w_0 * x + b_0)`<br>\n",
    "`x_2 = f(w_1 * x_1 + b_1)`<br>\n",
    "...<br>\n",
    "`y = f(w_n * x_n + b_n)`\n",
    "\n",
    "<br>\n",
    "\n",
    "donde la salida de una capa de neuronas es la entrada de la siguiente capa\n",
    "\n",
    "<br>\n",
    "\n",
    "![data/image-lect/ann.png](data/image-lect/ann.png)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 La función de _error_\n",
    "---\n",
    "Una diferencia importante entre nuestro modelo lineal precedente y el modelo usado en DL es la _forma_ de la función de error. Nuestro modelo lineal y la función de pérdida (_loss_) MSE, tenían una curva de error convexa que definía claramente un mínimo. Así, nosotros podemos actualizar nuestros parámetros en un intento de _estimar_ los valores que _minimicen_ el error y alcancen ese mínimo.\n",
    "\n",
    "Las redes neuronales no gozan de la misma propiedad de una superficie de error convexa. **No existe una única respuesta correcta para cada uno de los parámetros que estamos intentando aproximar**. Más bien, estamos buscando una **combinación de los parámetros** que produzcan los resultados deseados.\n",
    "\n",
    "Una de las razones por las que las redes neuronales no tienen superficies de error convexas es debido a la **función de activación**. Como contrapartida, la fantástica capacidad de las neuronas de aproximar tan amplio rango de funciones depende de la combinación del comportamiento lineal y no-lineal inherente a cada neurona."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 La función de _activación_\n",
    "---\n",
    "Como acabamos de describir, la unidad mínima de las redes neuonales profundas, la neurona, es una combinación de una operación lineal seguida por una función de activación. Ésta última realiza dos acciones importantes:\n",
    "\n",
    "- En las partes más internas (profundas) del modelo, permite a la función de salida tener diferentes pendientes en diferentes valores. Esto le permitirá a la red aproximar funciones arbitrarias por **composición** de funciones\n",
    "\n",
    "- En la última de las capas, juega el papel de **concentrar** las salidas de las operaciones lineales precedentes en un rango determinado\n",
    "\n",
    "En la siguiente imagen podemos apreciar el efecto de esto último, donde representamos la salida de un modelo de clasificación de imágenes de _dogo_\n",
    "\n",
    "<br>\n",
    "\n",
    "![data/image-lect/dog_activation.png](data/image-lect/dog_activation.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "Estas funciones de activación no-lineal (_sigmoid_, _tanh_, _relu_,...) nos permiten:\n",
    "\n",
    "- **Capar la salida** en un rango apropiado (0, 1), (-1-1),...\n",
    "- Presentan una zona de transición central que le permite a la neurona **ajustar** de forma fina la salida en base a los parámetros (pequeños cambios en éstos, tienen su reflejo en la salida resultante). En el ejemplo anterior, dos tipos de oso como el _grizzly_ y el _polar_ (que tiene una cara ligeramente más parecida a la de un perro), serían \"distinguidos\" por la neurona y recibirían puntuaciones diferentes sobre el eje Y, estando el _polar_ \"ligeramente\" más próximo al 1.\n",
    "\n",
    "Existen nuemerosas funciones que podemos emplear como función de activación. Por ejemplo:\n",
    "\n",
    "<br>\n",
    "\n",
    "![data/image-lect/activation_fun.png](data/image-lect/activation_fun.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "Actualmente, **ReLU** _(Rectified Linear Unit)_ es considerada una de las funciones de activación que proporciona mejores resultados. \n",
    "\n",
    "Por otro lado, **sigmoid**, ampliamente empleada en los trabajos iniciales de DL, ha caído en desuso excepto cuando expresamente nos queremos mover en el rango 0..1, por ejemplo, cuando la salida son probabilidades. \n",
    "\n",
    "Finalmente, **LeakyReLU** modifica la versión estándar de ReLU para presentar una pequeña pendiente para las entradas negativas, típicamente 0.01 (en lugar del valor estricto 0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3.1 Seleccionando la función de activación\n",
    "----\n",
    "Múltiples funciones de activación han provado su validez en diferentes contextos, por lo que no es fácil definir una serie de requerimientos que nos permitan decantarnos por una u otra.\n",
    "\n",
    "En general, una función de activación:\n",
    "\n",
    "- Es **no-lineal**. La aplicación repetida de funciones lineales sin una activación, resulta en una función de la misma forma (afín). La no linearidad es lo que permite a la red la aproximación de funciones complejas.\n",
    "\n",
    "<br>\n",
    "\n",
    "- Es **diferenciable**, por lo que se pueden calcular gradientes sobre ella\n",
    "\n",
    "<br>\n",
    "\n",
    "Sin esas dos características, la red se convertiría en un modelo lineal (incapaz de aproximar funciones complejas) o difícilmente entrenable.\n",
    "\n",
    "Por otro lado, su forma presenta también las siguientes características:\n",
    "\n",
    "- Tiene al menos un rango sensitivo, donde cambios en la entrada suponen cambios en la salida, algo necesario para el entrenamiento\n",
    "\n",
    "<br>\n",
    "\n",
    "- Tiene un rango insensitivo (saturado), donde cambios en la entrada no influyen (o apenas) en la salida. Esto va a tener su repercusión en el entrenamiento, pues los gradientes de estas zonas saturadas serán próximos a 0 resultando en poca (o nula) influencia en la actualización de los parámetros.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Qué significa que una red \"aprenda\"\n",
    "---\n",
    "Llegados a este punto, estamos empezando a tener la intuición de como, uniendo muchas unidades del tipo _función lineal + activación_ en paralelo (capa) y apilándolas unas sobre otras, construimos un **objeto matemático** capaz de de aproximar funciones complicadas o procesos altamente no lineales. Las diferentes combinaciones de unidades responderán a las entradas en rangos diferentes, siendo fácilmente entrenables mediante descenso de gradiente debido a su comportamiento prácticamente lineal en el rango sensible de la función de activación, incluso con millones de parámetros.\n",
    "\n",
    "En definitiva, lo que hace que el uso de redes neuronales profundas sea tan atractivo, es que nos abstrae de la naturaleza exacta de la función que representa nuestros datos (cuadrática, polinómica, ...). Con un modelo DL, tenemos un **aproximador universal** y un método de **estimación de parámetros**\n",
    "\n",
    "En la siguiente imagenmpodemos ver 4 neuronas (A, B, C, D), con su correspondiente función lineal _(w, b)_ y su activación (_tanh_)\n",
    "\n",
    "<br>\n",
    "\n",
    "![data/image-lect/aproximations.png](data/image-lect/aproximations.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "Las dos primeras columnas, en sentido vertical, representarían dos capas de dos neuronas cada, totalmente conectadas. Abajo a la derecha, tendríamos la única neurona de la tercera capa, resultando en una red: 2 x 2 x 1. Es importante ver como, a partir de funciones simples, podemos obtener la representación de la última función mucho más compleja. Al final, el entrenamiento consistirá en obtener los parámetros de nuestras neuronas que permitan aproximar la función buscada.\n",
    "\n",
    "Más importante aún es que, estas redes neuronales, al funcionar como aproximadores universales, nos permiten aproximar el modelo **sin necesidad de conocerlo** a partir exclusivamente de los datos experimentales. Esto es de vital importancia en numerosos problemas donde no conocemos dicho modelo o es extremadamente compleja su definición. Por el contrario, en nuestro ejemplo del termómetro, partimos de la idea de que el modelo buscado era lineal y planteamos nuestra solución en base a esa suposición. De no serlo, no nos habría funcionado. Ni nuestro modelo nos funcionará en otro caso que no sea similar al descrito.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
