{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aprendizaje automático II. Pytorch\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Autogradientes de PyTorch\n",
    "---\n",
    "En nuestro pequeñó experimento, vimos un ejemplo simple de propagación hacia atrás (_backpropagation_), el cálculo del gradiente de una composición de funciones (_model_ y _loss_) con respecto a sus parámetros (_w_ y _b_) mediante la propagación hacia atrás de sus derivadas parciales.\n",
    "\n",
    "Incluso aunque tuviéramos un modelo complicado con millones de parámetros, mientras el modelo sea diferenciable, definir el cálculo del gradiente de la función del error medido respecto de los parámetros no es más que un proceso analítico que debemos hacer una única vez. Eso sí, un proceso extremadamente aburrido y lento.\n",
    "\n",
    "Aquí es cuando PyTorch viene, de nuevo, al rescate con un componente denominado **_autograd_**. \n",
    "\n",
    "Los tensores de PyTorch pueden \"recordar\" de dónde proceden, es decir qué operaciones y tensores previos los originaron, y pueden proporcionar de forma automática la cadena de derivadas de dichas operaciones con respecto a sus entradas. Esto significa que ya no necesitamos derivar nuestro modelo _\"a mano\"_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Aplicando _autograd_\n",
    "---\n",
    "Vamos a recuperar nuestro modelo anterior del termómetro. Usaremos sobre él _autograd_ y veremos qué pasa.\n",
    "\n",
    "Primero, recuperamos el _dataset_..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# temperaturas ºC\n",
    "t_c = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0]\n",
    "\n",
    "# temperaturas en escala desconocida (unknown)\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "\n",
    "t_c = torch.tensor(t_c)\n",
    "t_u = torch.tensor(t_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y ahora nuestras funciones de modelo y pérdida..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u, w, b):\n",
    "    return w * t_u + b\n",
    "\n",
    "def loss_fn(t_p, t_c):\n",
    "    return ((t_c - t_p)**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a inicializar nuevamente nuestro tensor de parámetros pero, en este caso, le indicaremos a PyTorch (opción _requires_\\__grad_) que haga un seguimiento y registro de todas las operaciones (y tensores resultantes) que realicemos sobre el tensor. En caso de que dichas operaciones sean diferenciables, el valor de la derivada estará automáticamente disponible en el atributo **_grad_** del tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "params.grad is None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, lo que único que tenemos que hacer es calcular el modelo sobre unos datos de entrada, aplicar nuestra función de pérdida (_loss_, en nuestro caso, _MCE_) para computar la diferencia con respecto al valor esperado. Finalmente, invocar **_backward_** en el tensor resultante, de forma que el atributo _grad_ de _params_ contenga las derivadas de la función de pérdida respecto a cada elemento de parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4517.2969,   82.6000])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_p = model(t_u, *params)\n",
    "loss = loss_fn(t_p, t_c)\n",
    "loss.backward()\n",
    "\n",
    "params.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir del tensor original (_params_), PyTorch va creando un grafo con las diferentes operaciones realizadas sobre el tensor que, como resultado, producirán nuevos tensores. Cuando se invoca la función **_backward()_** sobre estos nuevos tensores, PyTorch atraviesa el grafo en orden inverso calculando las derivadas paraciales de las diferentes operaciones realizadas.\n",
    "\n",
    "NOTA: _backward_ sólo puede realizarse sobre tensores escalares (como ECM, cuyo resultado es un escalar)\n",
    "\n",
    "<br>\n",
    "\n",
    "![](data/image-lect/grad.png)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Reescribiendo nuestro bucle de entrenamiento\n",
    "---\n",
    "Vamos a reescribir nuestro bucle de entrenamiento. Nos apoyaremos en esta funcionalidad de PyTorch para calcular las derivadas parciales de la función de pérdida respecto a los parámetros y actualizar sus valores.\n",
    "\n",
    "Dado que las operaciones de _backward_ son acumulativas, deberemos tmar la precaución de \"resetear\" a 0 el gradiente del tensor antes de cada iteración\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c, print_params=False):\n",
    "    t_params_list = torch.zeros([n_epochs, 2])\n",
    "    t_loss_list = torch.zeros(n_epochs)\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        if params.grad is not None:\n",
    "            params.grad.zero_()\n",
    "                \n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        loss.backward()   # sustituimos la llamada a nuestras funciones de derivación\n",
    "        \n",
    "        # update parameters\n",
    "        with torch.no_grad():  # deshabilitamos autograd para operar con params\n",
    "            params -= learning_rate * params.grad\n",
    "        \n",
    "        # almacenamos los resultado de cada época para analizar\n",
    "        t_loss_list[epoch-1] = loss\n",
    "        t_params_list[epoch-1] = params # new params learned\n",
    "        \n",
    "        if print_params:\n",
    "            print(f'Epoch {epoch}: loss {loss:.3f}')  \n",
    "            print(f'\\tParams:\\t{params}:\\n\\tGrad:\\t{grad}')         \n",
    "    \n",
    "    return t_loss_list, t_params_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos si funciona..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.9276, grad_fn=<SelectBackward>) tensor([ 9.0349, 10.5000], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "t_un = (t_u - t_u.mean())/t_u.std()\n",
    "t_loss, t_params = training_loop(\n",
    "    n_epochs = 5000,\n",
    "    learning_rate = 1e-2,\n",
    "    params = params,\n",
    "    t_u = t_un,\n",
    "    t_c = t_c)\n",
    "print(t_loss[-1], t_params[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mismo resultado!! Ya no necesitamos calcular derivadas a mano!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Optimizadores\n",
    "---\n",
    "Existen diversas estrategis de optimización y \"trucos\" que pueden facilitar la convergencia, especialmente cuando tratamos con modelos complicados.\n",
    "\n",
    "PyTorch proporciona a través del módulo **_optim_** diferentes algoritmos de optimización que nos facilitan la tarea de actualización de los parámetros en cada iteración de entrenamiento, ahorrándonos el trabajo de hacerlo por nosotros mismos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASGD',\n",
       " 'Adadelta',\n",
       " 'Adagrad',\n",
       " 'Adam',\n",
       " 'AdamW',\n",
       " 'Adamax',\n",
       " 'LBFGS',\n",
       " 'Optimizer',\n",
       " 'RMSprop',\n",
       " 'Rprop',\n",
       " 'SGD',\n",
       " 'SparseAdam',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_multi_tensor',\n",
       " 'functional',\n",
       " 'lr_scheduler',\n",
       " 'swa_utils']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "dir(optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El constructor de cada uno de los optimizadores anteriores tomará una lista de parámetros como entrada (un tensor con el atributo _requires_\\__grad_ a _True_. Los parámetros serán mantenidos dentro del optimizador de forma que pueda actualizarlos y acceder al atributo _grad_.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](data/image-lect/optimizer.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "Cada optimizador expone dos métodos, _zero_\\__grad_ y _step. _zero_\\__grad_ \"resetea\" a 0 el atributo _grad_ de todos los parámetros recibidos. _step_ actualiza los valores de los parámetros de acuerdo a la estrategia de optimización seleccionada.\n",
    "\n",
    "Veamos con un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-5\n",
    "optimizer = optim.SGD([params], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_SGD_ significa _stochastic gradient descent_. El términoc _stochastic_ hace referencia a que  calcula el gradiente promediando sobre un _subset_ aleatorio de las muestras del _dataset_ (_minibatch_). \n",
    "\n",
    "En todo caso, el optimizador no sabe si está evaluando la función de pérdida sobre todo el _dataset_ o un subconjunto.\n",
    "\n",
    "Vamos a probarlo sobre nuestro _dataset_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9.5483e-01, -8.2600e-04], requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_p = model(t_u, *params)\n",
    "loss = loss_fn(t_p, t_c)\n",
    "loss.backward()\n",
    "\n",
    "optimizer.step()\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El valor de _params_ se ha actualizado después de invocar _step_ automáticamente! Del mismo modo que hacía nuestro código \"a mano\".\n",
    "\n",
    "Antes de modificar nuestra función de entrenamiento, debemos recordar inicializar a 0 nuestros gradientes o, de lo contrario, se acumularán en cada iteración del bucle cada vez que llamemos a _backward()_.\n",
    "\n",
    "Finalmente, nuestra función para el entrenamiento quedará de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, t_u, t_c, print_params=False):\n",
    "    t_params_list = torch.zeros([n_epochs, 2])\n",
    "    t_loss_list = torch.zeros(n_epochs)\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        t_p = model(t_u, *params) # aplica el modelo al dataset\n",
    "        loss = loss_fn(t_p, t_c)  # calcula el error\n",
    "\n",
    "        optimizer.zero_grad()  # inicializa los gradientes\n",
    "        loss.backward()        # calcula los gradientes\n",
    "        optimizer.step()       # actualiza los parámetros\n",
    "        \n",
    "        # almacenamos los resultado de cada época para analizar\n",
    "        t_loss_list[epoch-1] = loss\n",
    "        t_params_list[epoch-1] = params # new params learned\n",
    "        \n",
    "        if print_params:\n",
    "            print(f'Epoch {epoch}: loss {loss:.3f}')  \n",
    "            print(f'\\tParams:\\t{params}:\\n\\tGrad:\\t{grad}')         \n",
    "    \n",
    "    return t_loss_list, t_params_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y procederemos a entrenar nuestro modelo como siempre (fíjate que se pasa el _dataset_ normalizado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: 2.928; [w = 5.367, b = -17.301]\n"
     ]
    }
   ],
   "source": [
    "t_un = 0.1*t_u\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "t_loss, t_params = training_loop(\n",
    "    n_epochs = 5000,\n",
    "    optimizer = optimizer,\n",
    "    params = params,\n",
    "    t_u = t_un,\n",
    "    t_c = t_c)\n",
    "\n",
    "err = t_loss[-1].item()\n",
    "w,b = t_params[-1]\n",
    "print(f'error: {err:.3f}; [w = {w:.3f}, b = {b:.3f}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Probando otros optimizadores (ADAM)\n",
    "---\n",
    "Podemos probar fácilmente otros optimizadores. Basta con crear una nueva instancia del optimizador seleccionado y pasárselo a la función de entrenamiento.\n",
    "\n",
    "Por ejemplo, vamos a usar **_Adam_**. Este optimizador, más sofisticado que _SGD_, usa un _learning rate_ adaptativo y, por tanto, es menos sensible al escalado de los parámetros. Por ejemplo, vamos a emplearlo sobre los datos sin normalizar y con un _learnning rate_ de un orden de magnitud superior:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: 2.928; [w = 0.537, b = -17.305]\n"
     ]
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-1 # learning rate mayor\n",
    "optimizer = optim.Adam([params], lr=learning_rate) # Optimizador Adam\n",
    "\n",
    "t_loss, t_params = training_loop(\n",
    "    n_epochs = 5000,\n",
    "    optimizer = optimizer,\n",
    "    params = params,\n",
    "    t_u = t_u, # _dataset_ sin normalizar\n",
    "    t_c = t_c)\n",
    "\n",
    "err = t_loss[-1].item()\n",
    "w,b = t_params[-1]\n",
    "print(f'error: {err:.3f}; [w = {w:.3f}, b = {b:.3f}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mismo resultado!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 Entrenamiento, validación y _overfitting_\n",
    "---\n",
    "Como hemos visto, cuando diseñamos y entrenamos un modelo, básicamente estamos buscando los valores de una serie de parámetros que nos permitan **_aproximar_** una determinada función, lo que nos permitirá hacer predicciones futuras a partir de datos de entrada no vistos durante el entrenamiento.\n",
    "\n",
    "Cuando nuestro modelo es extremandamente flexible y adaptable, como el proporcionado por las redes neuronales profundas, podemos _virtualmente_ aproximar cualquier función. Es decir, estos modelos de múltiples parámetros son capaces de ajustarse de forma que la pérdida sea mínima en las muestras del _dataset_ de entrenamiento. Pero esto no nos garantiza que el comportamiento del modelo sea correcto con muestras \"desconocidas\" o \"no vistas\" previamente, lo que invalidaría su uso como predictor. Este fenómeno es lo que se conoce como **_overfitting_**. El modelo ha sido capaz de predecir perfectamente los resultados esperados en el _dataset_ de entrenamiento, ajustando los parámetros hasta minimizar las pérdidas al máximo, pero con un comportamiento muy pobre con datos nuevos.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](data/image-lect/overfitting.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "Un fenómeno contrario es el denominado **_underfitting_**. En este caso, no conseguimos disminuir la pérdida con el entrenamiento. Esto puede ser debido a dos razones fundamentales:\n",
    "\n",
    "1. El modelo es demasiado simple (pocos parámetros) y no consigue aproximar la función \n",
    "2. El _dataset_ de entrenamiento es pobre, bien porque contiene pocas muestras o porque los datos contenidos no aportan suficiente información para \"descubrir\" relaciones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### _Training_\\__set_ y _Validation_\\__set_\n",
    "---\n",
    "Con objeto de poder validar el comportamiento de nuestro modelo, antes de proceder a sus entrenamiento, deberemos dividir el _dataset_ original (de forma aleatoria) en dos conjuntos independientes: el **_training_\\__set_** y el **_validation_\\__set_**\n",
    "\n",
    "Como su nombre indica, el primero lo utilizaremos para entrenar el modelo, mientra que el segundo sólo se utilizará para evaluar sus predicciones sobre conjuntos de datos no vistos previamente, lo que nos dará una idea de su comportamiento real. \n",
    "\n",
    "<br>\n",
    "\n",
    "![data/image-lect/split.png](data/image-lect/split.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "Generalmente, el _validation_\\__set_ suele tener un tamaño en torno al 20% del _dataset_ original.\n",
    "\n",
    "Podemos hacer la división del _dataset_ fácilmente creando un vector con los índices del _dataset_ ordenados al azar (_torch.randperm_) y emplearlo para extraer dos subconjuntos diferenciados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = t_u.shape[0]\n",
    "n_val = int(0.2 * n_samples)\n",
    "\n",
    "shuffled_indices = torch.randperm(n_samples)\n",
    "\n",
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez creados los dos conjuntos de índices, los usaremos para construir los dos nuevos _datasets_ a partir del original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training dataset\n",
    "train_t_u = t_u[train_indices]\n",
    "train_t_c = t_c[train_indices]\n",
    "\n",
    "# validation dataset\n",
    "val_t_u = t_u[val_indices]\n",
    "val_t_c = t_c[val_indices]\n",
    "\n",
    "# normalization (if necessary)\n",
    "train_t_un = 0.1 * train_t_u\n",
    "val_t_un = 0.1 * val_t_u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora procederíamos a entrenar nuestro modelo sobre el _training_\\__dataset_ para, posteriormente, una vez determinados los parámetros, evaluar su rendimiento sobre el _validation_\\__dataset_\n",
    "\n",
    "Dado que nuestra función de entrenamiento nos devuelve los parámetros de cada iteración, podemos evaluar cada época y ver como se ha ido decreciendo el error en el _validation_\\__dataset_ (esto mismo lo podíamos hacer añadiendo la validación dentro del bucle de entrenamiento y devolviendo un nuevo valor, el _error_ en el _validation_\\__dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: 2.733; [w = 5.315, b = -17.421]\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "t_loss, t_params = training_loop(\n",
    "    n_epochs = 5000,\n",
    "    optimizer = optimizer,\n",
    "    params = params,\n",
    "    t_u = train_t_un,   # training data (samples)\n",
    "    t_c = train_t_c)    # training data (labels)\n",
    "\n",
    "err = t_loss[-1].item()\n",
    "w,b = t_params[-1]\n",
    "print(f'error: {err:.3f}; [w = {w:.3f}, b = {b:.3f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: 4.672;\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "t_val_loss = torch.zeros(t_params.size()[0])\n",
    "with torch.no_grad(): # podemos deshabilitar el cálculo de gradientes (nota final)\n",
    "    for i, params in enumerate(t_params):\n",
    "        val_t_p = model(val_t_un, *params)\n",
    "        t_val_loss[i] = loss_fn(val_t_p, val_t_c)\n",
    "    \n",
    "err = t_val_loss[-1].item()\n",
    "print(f'error: {err:.3f};')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, existe una pequeña discrepancia entre el error final _training_\\__set_ y en el _validation_\\__set_. Este error, como es lógico, siempre será mayor en la validación. De todas formas, lo que realmente nos importa es ver que el error vaya descendiendo a lo largo del proceso de entrenamiento.\n",
    "\n",
    "Vamos a imprimir la evolución de ambos errores (en el _training_\\__set_ y en el _validation_\\__set_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "t_x = torch.arange(t_loss.size()[0])\n",
    "plt.plot(t_loss.detach().numpy(), label = 'training') # detach().numpy() porque tiene gradientes\n",
    "plt.plot(t_val_loss, label = 'validation') # no tiene gradientes. No necesitamos convertirlo\n",
    "plt.title(f\"Curva de aprendizaje\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.xlabel(\"Época\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque el ejemplo propuesto no es muy representativo, pues el _validation_\\__set_ es demasiado pequeño, podemos apreciar el descenso en ambos _datasets_ a medida que avanza el entrenamiento.\n",
    "\n",
    "El estudio de estas gráficas comparadas es tremendamente útil, tal como podemos apreciar en la siguiente imagen. Nos permitirá, tanto observar comportamientos ideales (C) o aceptables (D), como detectar fácilmente situaciones de _underfitting_ (A) como de _overfitting_ (B).\n",
    "\n",
    "<br>\n",
    "\n",
    "![data/image-lect/train_vs_val_loss.png](data/image-lect/train_vs_val_loss.png)\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NOTA: Autograd switch off\n",
    "---\n",
    "Una pequeña nota final sobre el proceso anterior de evaluación. Debemos tener presente que se van a crear dos grafos independientes a partir de _params_.\n",
    "\n",
    "<br>\n",
    "\n",
    "![data/image-lect/params_graph.png](data/image-lect/params_graph.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "Dado que sólo entrenamos sobre el _training_\\__set_, sólo vamos a realizar _backpropagation_ en las operaciones sobre este _set_. Por tanto, no necesitamos el segundo grafo para nada y podemos desactivar _autograd_ haciendo uso de _torch.no_\\__grad_ en la validación\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
