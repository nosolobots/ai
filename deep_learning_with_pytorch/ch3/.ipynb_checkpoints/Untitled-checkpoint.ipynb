{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch tensors\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensors** are the fundamental data structure in PyTorch. A tensor is an array, that is, a data structure tht stores a collection of numbers that are accesible individually using an index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.])\n",
      "tensor(1.)\n",
      "tensor([1., 1., 2.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.ones(3)\n",
    "print(a)\n",
    "print(a[1])\n",
    "a[2] = 2.\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python lists or tuples are collections of Python objects that are individually allocated in memory. PyTorch (or NumPy) tensors, on the oder hand, are contiguous memory blocks containing _unboxed_ C numeric types rather than Python objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4.,  1.,  3., -1.,  0.,  2.])\n",
      "<class 'torch.Tensor'>\n",
      "torch.float32\n",
      "tensor([ 4.,  1.,  3., -1.,  0.,  2.])\n",
      "torch.Size([6])\n",
      "tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "points = torch.tensor([4.0, 1.0, 3.0, -1.0, 0.0, 2.0])\n",
    "print(points)\n",
    "print(type(points))\n",
    "print(points.dtype)\n",
    "print(points)\n",
    "print(points.shape)\n",
    "print(points[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4,  1,  3, -1], dtype=torch.int32)\n",
      "<class 'torch.Tensor'>\n",
      "torch.int32\n",
      "tensor([[ 4.,  1.],\n",
      "        [ 3., -1.],\n",
      "        [ 0.,  2.]])\n",
      "torch.Size([3, 2])\n",
      "tensor([4., 1.])\n"
     ]
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [3.0, -1.0], [0.0, 2.0]])\n",
    "print(pointsi)\n",
    "print(type(pointsi))\n",
    "print(pointsi.dtype)\n",
    "print(points)\n",
    "print(points.shape)\n",
    "print(points[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.,  1.],\n",
      "        [ 3., -1.],\n",
      "        [ 0.,  2.]])\n",
      "tensor([[ 3., -1.],\n",
      "        [ 0.,  2.]])\n",
      "tensor([[ 3., -1.],\n",
      "        [ 0.,  2.]])\n",
      "tensor([3., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(points)\n",
    "print(points[1:]) # all rows after the first; implicitly all columns\n",
    "print(points[1:, :]) # all rows after the first; all columns\n",
    "print(points[1:, 0]) # all rows after the first; first column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named tensors\n",
    "As data is transformed through multiple tensors, keeping track of whch dimension contains what data can be error-prone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_t:\n",
      " tensor([[[ 5.9593e-01, -2.2554e-01,  1.3033e+00,  4.3092e-01, -1.1777e-02],\n",
      "         [ 8.8068e-01,  1.6422e-02, -1.3252e-01, -6.3991e-01,  2.4052e-01],\n",
      "         [-1.1630e+00,  9.7681e-02,  1.3905e+00, -3.4770e-01,  2.4636e-01],\n",
      "         [ 1.5399e-01,  1.2837e+00, -3.7645e-01, -9.6737e-02,  6.8970e-01],\n",
      "         [ 1.0004e-03, -1.0694e+00, -3.7071e-01, -4.7061e-01,  6.0313e-01]],\n",
      "\n",
      "        [[-7.3865e-01, -6.0208e-01, -6.4794e-01, -3.4129e-01, -5.4593e-01],\n",
      "         [ 4.7147e-02,  3.0631e-01, -2.8535e-02, -2.3648e+00, -1.2492e+00],\n",
      "         [ 2.5260e-01,  1.3576e+00,  2.2554e+00,  1.2114e+00, -1.5182e+00],\n",
      "         [-1.2190e+00, -1.3453e+00,  2.6575e+00, -5.4048e-01,  5.1252e-01],\n",
      "         [ 1.4009e+00, -8.7529e-01,  1.5244e-01, -2.9810e-01,  1.1409e+00]],\n",
      "\n",
      "        [[ 1.4830e+00, -6.1663e-01,  9.3938e-02, -1.0401e+00, -1.3305e+00],\n",
      "         [ 3.1038e-02, -1.4777e+00, -1.1980e+00, -1.4419e+00,  1.0255e+00],\n",
      "         [ 4.6934e-01, -4.5564e-01, -1.4515e+00, -1.7802e+00,  1.6938e-01],\n",
      "         [ 1.4895e-01,  3.9539e-01, -8.9248e-01, -1.8874e+00, -1.3285e+00],\n",
      "         [-2.0735e-01,  1.7043e-01,  1.2400e+00, -4.6769e-01, -1.6046e+00]]])\n",
      "weights:\n",
      " tensor([0.2126, 0.7152, 0.0722])\n"
     ]
    }
   ],
   "source": [
    "img_t = torch.randn(3, 5, 5) # shape [channels, rows, columns]\n",
    "print(\"img_t:\\n\", img_t)\n",
    "\n",
    "weights = torch.tensor([0.2126, 0.7152, 0.0722])\n",
    "print(\"weights:\\n\", weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_t:\n",
      " tensor([[[[-1.4247e+00,  1.1949e+00, -4.2283e-01, -5.4617e-01, -1.1286e+00],\n",
      "          [ 8.6600e-01, -1.0298e-01, -1.4553e+00, -4.5132e-01, -3.2064e-01],\n",
      "          [ 2.7672e+00, -1.8659e+00, -7.2636e-01, -1.0149e+00, -1.2483e-01],\n",
      "          [-1.8447e+00, -6.8048e-01,  9.1462e-01, -1.9755e+00, -1.1569e+00],\n",
      "          [-7.0247e-01,  4.6351e-01,  1.5625e+00, -7.5395e-01, -8.2287e-01]],\n",
      "\n",
      "         [[ 1.0138e+00, -1.2193e+00,  4.0404e-01, -1.1294e+00,  1.0647e+00],\n",
      "          [-1.1283e+00,  6.5247e-01,  1.0382e+00, -2.0373e+00, -1.3780e-01],\n",
      "          [ 3.0119e-01, -5.1570e-01, -9.5104e-01,  5.8515e-01, -1.8407e+00],\n",
      "          [-9.5803e-01, -1.1343e-01,  9.0483e-01,  9.4589e-01,  1.4644e-01],\n",
      "          [ 4.4431e-01, -1.3358e-02,  1.1697e+00, -2.4152e+00, -8.1803e-01]],\n",
      "\n",
      "         [[-1.0996e+00,  3.0452e+00,  1.5221e+00, -1.7188e-01,  5.8116e-01],\n",
      "          [-7.1621e-01,  1.5814e+00,  6.0045e-01, -1.7724e+00,  2.4083e+00],\n",
      "          [-7.7640e-01, -4.4629e-01,  9.4052e-01, -1.2211e+00, -8.6813e-02],\n",
      "          [-2.4110e-01,  9.8916e-01,  1.4318e+00,  1.0487e+00,  1.1090e+00],\n",
      "          [-5.1606e-01,  5.3886e-01, -1.5282e-01, -2.0058e-01,  4.8142e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5737e-01,  6.1504e-01, -4.9582e-02,  7.4686e-01, -3.5843e-01],\n",
      "          [ 1.2757e+00, -4.3296e-01,  4.3916e-01, -6.0191e-01,  1.1314e-01],\n",
      "          [-9.4534e-01,  1.1561e+00,  3.1491e-01, -1.3729e-01, -9.3970e-01],\n",
      "          [ 1.2521e+00, -1.8288e-04,  2.1547e+00, -1.4637e+00, -2.1911e-01],\n",
      "          [ 1.0518e+00,  9.6538e-01, -1.9324e+00, -5.7856e-01,  6.4942e-01]],\n",
      "\n",
      "         [[-5.1375e-01,  1.4465e+00,  3.9702e-02,  9.9135e-01,  1.1607e+00],\n",
      "          [ 1.0785e+00,  1.1888e+00,  2.0600e-04,  1.6713e+00, -1.3460e+00],\n",
      "          [ 1.7938e-01,  8.1821e-01, -1.0136e+00,  2.7675e-01, -1.3470e-01],\n",
      "          [ 1.4783e-01, -4.0270e-01,  3.0170e-01, -1.3581e+00, -6.7565e-01],\n",
      "          [-1.2289e+00,  5.0543e-01, -6.8669e-01,  1.3667e+00,  2.0134e-01]],\n",
      "\n",
      "         [[-9.0922e-02,  1.1836e+00,  1.5368e+00, -4.6229e-01,  4.4338e-01],\n",
      "          [ 2.1061e+00, -6.0931e-01, -8.5317e-01, -1.1532e+00, -6.0352e-01],\n",
      "          [-1.8328e+00, -4.1350e-01, -2.4637e-01, -2.2549e-01,  1.2903e-01],\n",
      "          [-1.1255e+00, -3.4965e-02, -1.4293e+00,  8.2939e-01,  2.3481e-01],\n",
      "          [-5.7943e-01,  1.7035e-01,  8.7976e-01,  4.4166e-01, -3.0107e-01]]]])\n"
     ]
    }
   ],
   "source": [
    "# multiple images\n",
    "batch_t = torch.randn(2, 3, 5, 5) # shape [batch, channels, rows, columns]\n",
    "print(\"img_t:\\n\", batch_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw, the RGB channels are in dimension 0 in the first case and in dimension 1 in the second. We could obtain the unweighted mean using dimension -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4468, -0.4814,  0.2498, -0.3168, -0.6294],\n",
      "        [ 0.3196, -0.3850, -0.4530, -1.4822,  0.0056],\n",
      "        [-0.1470,  0.3332,  0.7314, -0.3055, -0.3675],\n",
      "        [-0.3054,  0.1112,  0.4628, -0.8415, -0.0421],\n",
      "        [ 0.3982, -0.5914,  0.3406, -0.4121,  0.0465]]) torch.Size([5, 5])\n",
      "tensor([[[-0.5035,  1.0069,  0.5011, -0.6158,  0.1724],\n",
      "         [-0.3262,  0.7103,  0.0611, -1.4203,  0.6500],\n",
      "         [ 0.7640, -0.9426, -0.2456, -0.5503, -0.6841],\n",
      "         [-1.0146,  0.0651,  1.0837,  0.0064,  0.0328],\n",
      "         [-0.2581,  0.3297,  0.8598, -1.1232, -0.3865]],\n",
      "\n",
      "        [[-0.1491,  1.0817,  0.5090,  0.4253,  0.4152],\n",
      "         [ 1.4867,  0.0488, -0.1379, -0.0279, -0.6121],\n",
      "         [-0.8663,  0.5203, -0.3150, -0.0287, -0.3151],\n",
      "         [ 0.0915, -0.1459,  0.3424, -0.6641, -0.2200],\n",
      "         [-0.2522,  0.5471, -0.5798,  0.4099,  0.1832]]]) torch.Size([2, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "img_gray_naive = img_t.mean(-3)\n",
    "batch_t_naive = batch_t.mean(-3)\n",
    "print(img_gray_naive, img_gray_naive.shape)\n",
    "print(batch_t_naive, batch_t_naive.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working whith numeric indexes for dimensions can be messy... Better, use names. We can add names to an existing tensor using the method _refine_\\__names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_named: torch.Size([3, 5, 5]) ('channels', 'rows', 'columns')\n",
      "batch_named: torch.Size([2, 3, 5, 5]) ('batch_id', 'channels', 'rows', 'columns')\n",
      "weights_named: torch.Size([3]) ('channels',)\n"
     ]
    }
   ],
   "source": [
    "img_named = img_t.refine_names(..., 'channels', 'rows', 'columns')\n",
    "batch_named = batch_t.refine_names(..., 'batch_id', 'channels', 'rows', 'columns')\n",
    "weights_named = weights.refine_names(..., 'channels')\n",
    "print(\"img_named:\", img_named.shape, img_named.names)\n",
    "print(\"batch_named:\", batch_named.shape, batch_named.names)\n",
    "print(\"weights_named:\", weights_named.shape, weights_named.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights_aligned: torch.Size([3, 1, 1]) ('channels', 'rows', 'columns')\n"
     ]
    }
   ],
   "source": [
    "weights_aligned = weights_named.align_as(img_named)\n",
    "print(\"weights_aligned:\", weights_aligned.shape, weights_aligned.names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions accepting dimension arguments also take named dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5]) ('rows', 'columns')\n"
     ]
    }
   ],
   "source": [
    "gray_named = (img_named * weights_aligned).sum('channels')\n",
    "print(gray_named.shape, gray_named.names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to use tensors outside functions that operate on named tensors, we need to drop the names by renaming them to _None_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gray_plain: torch.Size([5, 5]) (None, None)\n"
     ]
    }
   ],
   "source": [
    "gray_plain = gray_named.rename(None)\n",
    "print(\"gray_plain:\", gray_plain.shape, gray_plain.names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
