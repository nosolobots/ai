{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch tensors\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensors** are the fundamental data structure in PyTorch. A tensor is an array, that is, a data structure that stores a collection of numbers that are accesible individually using an index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.])\n",
      "tensor(1.)\n",
      "tensor([1., 1., 2.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.ones(3)\n",
    "print(a)\n",
    "print(a[1])\n",
    "a[2] = 2.\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python lists or tuples are collections of Python objects that are individually allocated in memory. PyTorch (or NumPy) tensors, on the oder hand, are contiguous memory blocks containing _unboxed_ C numeric types rather than Python objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4.,  1.,  3., -1.,  0.,  2.])\n",
      "<class 'torch.Tensor'>\n",
      "torch.float32\n",
      "torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "points = torch.tensor([4.0, 1.0, 3.0, -1.0, 0.0, 2.0])\n",
    "print(points)\n",
    "print(type(points))\n",
    "print(points.dtype)\n",
    "print(points.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.,  1.],\n",
      "        [ 3., -1.],\n",
      "        [ 0.,  2.]])\n",
      "<class 'torch.Tensor'>\n",
      "torch.float32\n",
      "torch.Size([3, 2])\n",
      "tensor([4., 1.])\n"
     ]
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [3.0, -1.0], [0.0, 2.0]])\n",
    "print(points)\n",
    "print(type(points))\n",
    "print(points.dtype)\n",
    "print(points.shape)\n",
    "print(points[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.,  1.],\n",
      "        [ 3., -1.],\n",
      "        [ 0.,  2.]])\n",
      "tensor([[ 3., -1.],\n",
      "        [ 0.,  2.]])\n",
      "tensor([[ 3., -1.],\n",
      "        [ 0.,  2.]])\n",
      "tensor([3., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(points)\n",
    "print(points[1:]) # all rows after the first; implicitly all columns\n",
    "print(points[1:, :]) # all rows after the first; all columns\n",
    "print(points[1:, 0]) # all rows after the first; first column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named tensors\n",
    "As data is transformed through multiple tensors, keeping track of which dimension contains what data can be error-prone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_t:\n",
      " tensor([[[-0.0258, -1.2125,  0.8376, -0.6120,  2.0765],\n",
      "         [ 0.2579,  1.1854, -1.5875, -0.3985, -1.0811],\n",
      "         [-1.2242,  1.7786, -1.3966,  0.8025,  1.2879],\n",
      "         [ 0.3271, -0.6721,  0.0880, -0.6468,  0.5000],\n",
      "         [-0.9931, -0.3810,  1.5752,  0.4030, -0.9618]],\n",
      "\n",
      "        [[ 0.4382,  0.9779, -1.3235,  0.0557, -0.0860],\n",
      "         [-0.9753,  1.3039,  1.4092, -1.3280,  0.5435],\n",
      "         [ 0.2879,  0.5714,  0.4650, -0.2193, -1.3085],\n",
      "         [-0.9611, -0.2746,  1.3587, -0.2521, -0.1916],\n",
      "         [ 0.8649,  0.8873,  0.5976, -0.4790, -0.2231]],\n",
      "\n",
      "        [[ 0.2584, -0.9934, -1.0889, -1.2935,  0.0070],\n",
      "         [ 0.3049, -0.7106,  0.0565, -0.2007,  1.0209],\n",
      "         [ 0.3737,  2.0778,  1.2476, -0.7243,  1.1933],\n",
      "         [-1.3843,  0.9369,  1.5409,  0.6231,  0.6172],\n",
      "         [ 1.4078, -0.4786,  0.1354,  0.1659, -0.1126]]])\n",
      "weights:\n",
      " tensor([0.2126, 0.7152, 0.0722])\n"
     ]
    }
   ],
   "source": [
    "img_t = torch.randn(3, 5, 5) # shape [channels, rows, columns]\n",
    "print(\"img_t:\\n\", img_t)\n",
    "\n",
    "weights = torch.tensor([0.2126, 0.7152, 0.0722])\n",
    "print(\"weights:\\n\", weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_t:\n",
      " tensor([[[[-1.4247e+00,  1.1949e+00, -4.2283e-01, -5.4617e-01, -1.1286e+00],\n",
      "          [ 8.6600e-01, -1.0298e-01, -1.4553e+00, -4.5132e-01, -3.2064e-01],\n",
      "          [ 2.7672e+00, -1.8659e+00, -7.2636e-01, -1.0149e+00, -1.2483e-01],\n",
      "          [-1.8447e+00, -6.8048e-01,  9.1462e-01, -1.9755e+00, -1.1569e+00],\n",
      "          [-7.0247e-01,  4.6351e-01,  1.5625e+00, -7.5395e-01, -8.2287e-01]],\n",
      "\n",
      "         [[ 1.0138e+00, -1.2193e+00,  4.0404e-01, -1.1294e+00,  1.0647e+00],\n",
      "          [-1.1283e+00,  6.5247e-01,  1.0382e+00, -2.0373e+00, -1.3780e-01],\n",
      "          [ 3.0119e-01, -5.1570e-01, -9.5104e-01,  5.8515e-01, -1.8407e+00],\n",
      "          [-9.5803e-01, -1.1343e-01,  9.0483e-01,  9.4589e-01,  1.4644e-01],\n",
      "          [ 4.4431e-01, -1.3358e-02,  1.1697e+00, -2.4152e+00, -8.1803e-01]],\n",
      "\n",
      "         [[-1.0996e+00,  3.0452e+00,  1.5221e+00, -1.7188e-01,  5.8116e-01],\n",
      "          [-7.1621e-01,  1.5814e+00,  6.0045e-01, -1.7724e+00,  2.4083e+00],\n",
      "          [-7.7640e-01, -4.4629e-01,  9.4052e-01, -1.2211e+00, -8.6813e-02],\n",
      "          [-2.4110e-01,  9.8916e-01,  1.4318e+00,  1.0487e+00,  1.1090e+00],\n",
      "          [-5.1606e-01,  5.3886e-01, -1.5282e-01, -2.0058e-01,  4.8142e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5737e-01,  6.1504e-01, -4.9582e-02,  7.4686e-01, -3.5843e-01],\n",
      "          [ 1.2757e+00, -4.3296e-01,  4.3916e-01, -6.0191e-01,  1.1314e-01],\n",
      "          [-9.4534e-01,  1.1561e+00,  3.1491e-01, -1.3729e-01, -9.3970e-01],\n",
      "          [ 1.2521e+00, -1.8288e-04,  2.1547e+00, -1.4637e+00, -2.1911e-01],\n",
      "          [ 1.0518e+00,  9.6538e-01, -1.9324e+00, -5.7856e-01,  6.4942e-01]],\n",
      "\n",
      "         [[-5.1375e-01,  1.4465e+00,  3.9702e-02,  9.9135e-01,  1.1607e+00],\n",
      "          [ 1.0785e+00,  1.1888e+00,  2.0600e-04,  1.6713e+00, -1.3460e+00],\n",
      "          [ 1.7938e-01,  8.1821e-01, -1.0136e+00,  2.7675e-01, -1.3470e-01],\n",
      "          [ 1.4783e-01, -4.0270e-01,  3.0170e-01, -1.3581e+00, -6.7565e-01],\n",
      "          [-1.2289e+00,  5.0543e-01, -6.8669e-01,  1.3667e+00,  2.0134e-01]],\n",
      "\n",
      "         [[-9.0922e-02,  1.1836e+00,  1.5368e+00, -4.6229e-01,  4.4338e-01],\n",
      "          [ 2.1061e+00, -6.0931e-01, -8.5317e-01, -1.1532e+00, -6.0352e-01],\n",
      "          [-1.8328e+00, -4.1350e-01, -2.4637e-01, -2.2549e-01,  1.2903e-01],\n",
      "          [-1.1255e+00, -3.4965e-02, -1.4293e+00,  8.2939e-01,  2.3481e-01],\n",
      "          [-5.7943e-01,  1.7035e-01,  8.7976e-01,  4.4166e-01, -3.0107e-01]]]])\n"
     ]
    }
   ],
   "source": [
    "# multiple images\n",
    "batch_t = torch.randn(2, 3, 5, 5) # shape [batch, channels, rows, columns]\n",
    "print(\"img_t:\\n\", batch_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw, the RGB channels are in dimension 0 in the first case and in dimension 1 in the second. We could obtain the unweighted mean using dimension -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4468, -0.4814,  0.2498, -0.3168, -0.6294],\n",
      "        [ 0.3196, -0.3850, -0.4530, -1.4822,  0.0056],\n",
      "        [-0.1470,  0.3332,  0.7314, -0.3055, -0.3675],\n",
      "        [-0.3054,  0.1112,  0.4628, -0.8415, -0.0421],\n",
      "        [ 0.3982, -0.5914,  0.3406, -0.4121,  0.0465]]) torch.Size([5, 5])\n",
      "tensor([[[-0.5035,  1.0069,  0.5011, -0.6158,  0.1724],\n",
      "         [-0.3262,  0.7103,  0.0611, -1.4203,  0.6500],\n",
      "         [ 0.7640, -0.9426, -0.2456, -0.5503, -0.6841],\n",
      "         [-1.0146,  0.0651,  1.0837,  0.0064,  0.0328],\n",
      "         [-0.2581,  0.3297,  0.8598, -1.1232, -0.3865]],\n",
      "\n",
      "        [[-0.1491,  1.0817,  0.5090,  0.4253,  0.4152],\n",
      "         [ 1.4867,  0.0488, -0.1379, -0.0279, -0.6121],\n",
      "         [-0.8663,  0.5203, -0.3150, -0.0287, -0.3151],\n",
      "         [ 0.0915, -0.1459,  0.3424, -0.6641, -0.2200],\n",
      "         [-0.2522,  0.5471, -0.5798,  0.4099,  0.1832]]]) torch.Size([2, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "img_gray_naive = img_t.mean(-3)\n",
    "batch_t_naive = batch_t.mean(-3)\n",
    "print(img_gray_naive, img_gray_naive.shape)\n",
    "print(batch_t_naive, batch_t_naive.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working whith numeric indexes for dimensions can be messy... Better, use names. We can add names to an existing tensor using the method _refine_\\__names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_named: torch.Size([3, 5, 5]) ('channels', 'rows', 'columns')\n",
      "batch_named: torch.Size([2, 3, 5, 5]) ('batch_id', 'channels', 'rows', 'columns')\n",
      "weights_named: torch.Size([3]) ('channels',)\n"
     ]
    }
   ],
   "source": [
    "img_named = img_t.refine_names(..., 'channels', 'rows', 'columns')\n",
    "batch_named = batch_t.refine_names(..., 'batch_id', 'channels', 'rows', 'columns')\n",
    "weights_named = weights.refine_names(..., 'channels')\n",
    "print(\"img_named:\", img_named.shape, img_named.names)\n",
    "print(\"batch_named:\", batch_named.shape, batch_named.names)\n",
    "print(\"weights_named:\", weights_named.shape, weights_named.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights_aligned: torch.Size([3, 1, 1]) ('channels', 'rows', 'columns')\n"
     ]
    }
   ],
   "source": [
    "weights_aligned = weights_named.align_as(img_named)\n",
    "print(\"weights_aligned:\", weights_aligned.shape, weights_aligned.names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions accepting dimension arguments also take named dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5]) ('rows', 'columns')\n"
     ]
    }
   ],
   "source": [
    "gray_named = (img_named * weights_aligned).sum('channels')\n",
    "print(gray_named.shape, gray_named.names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to use tensors outside functions that operate on named tensors, we need to drop the names by renaming them to _None_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gray_plain: torch.Size([5, 5]) (None, None)\n"
     ]
    }
   ],
   "source": [
    "gray_plain = gray_named.rename(None)\n",
    "print(\"gray_plain:\", gray_plain.shape, gray_plain.names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor data types\n",
    "Computations in neural networks are typically executed with **32-bit floating-point** precision. For indexing, PyTorch expects indexing tensors to have a **64-bit integer** data type. So, the most commonly used data types are _float32_ and _int64_.\n",
    "\n",
    "In order to allocate a tensor of the right numeric type, we can specify the proper _dtype_ as an argument to the constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "double_points = torch.ones(5, 2, dtype=torch.double)\n",
    "print(double_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos hacer un _cast_ al tipo deseado usando el método correspondiente o empleando el método _to()_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]], dtype=torch.float64)\n",
      "tensor([[1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1]], dtype=torch.int16)\n"
     ]
    }
   ],
   "source": [
    "double_points = torch.ones(5, 2).double()\n",
    "print(double_points)\n",
    "short_points = double_points.to(dtype=torch.short)\n",
    "print(short_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When mixing data types in operations, the operands are converted to the larger type automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2.],\n",
      "        [2., 2.],\n",
      "        [2., 2.],\n",
      "        [2., 2.],\n",
      "        [2., 2.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "points_sum = double_points+short_points\n",
    "print(points_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor API\n",
    "PyTorch offers a lot of operations that we can perform on and between tensors. The vast mayority are available as functions in the _torch_ module ([https://pytorch.org/docs/stable/torch.html](https://pytorch.org/docs/stable/torch.html)) and can also be called as methods of a tensor object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(3, 2)\n",
    "print(a)\n",
    "a_t = torch.transpose(a, 0, 1)\n",
    "print(a_t)\n",
    "a_t2 = a.transpose(0, 1)\n",
    "print(a_t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor metadata: storage, size, offset, stride\n",
    "Values in tensors are allocated in contiguous chunks of memory (one-dimensional array) managed by _torch.Storage_ instances. A _Tensor_ instance is a view of such array that is capable of indexing into that storage using dimension indexes. \n",
    "\n",
    "Multiple tensors can index the same storage even if they index into the data differently.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](img/storage.png)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have acces to the underlying array using the _storage_ property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 4.0\n",
       " 1.0\n",
       " 5.0\n",
       " 3.0\n",
       " 2.0\n",
       " 1.0\n",
       "[torch.FloatStorage of size 6]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4., 1.], [5., 3.], [2., 1.]])\n",
    "points.storage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have access to the every single element by indexing the storage array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n",
      "tensor([[ 4., 66.],\n",
      "        [99.,  3.],\n",
      "        [ 2.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "arr_p = points.storage()\n",
    "print(arr_p[2])\n",
    "arr_p[2] = 99.\n",
    "points.storage()[1] = 66.\n",
    "print(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to index into a storage, tensors rely on a few pieces of information: size, offset and stride.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](img/offset_stride.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "The **size** (shape) is a tuple indicating de number of elements in each dimension of the tensor. The **offset**, is the index in the storage corresponding to the first element in the tensor. The **stride** is the number of elements in the storage that need to be skipped over to obtain the next element along each dimension.\n",
    "\n",
    "For example, accesing an element i, j in a 2D tensor results in accessing:\n",
    "\n",
    "_storage_\\__offset_ + _stride_\\[0\\]\\*i + _stride_\\[1\\]\\*j\n",
    "\n",
    "This kind of indirection makes some operations inexpensive, like transposing a tensor or extracting a subtensor, because they do not lead to memory reallocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 1.],\n",
      "        [5., 3.],\n",
      "        [2., 1.]])\n",
      "points stride: (2, 1)\n",
      "tensor([5., 3.])\n",
      "second_point size: torch.Size([2])\n",
      "second_point offset: 2\n"
     ]
    }
   ],
   "source": [
    "points = torch.tensor([[4., 1.], [5., 3.], [2., 1.]])\n",
    "print(points)\n",
    "print(\"points stride:\", points.stride())\n",
    "\n",
    "second_point = points[1]\n",
    "print(second_point)\n",
    "print(\"second_point size:\", second_point.size())\n",
    "print(\"second_point offset:\", second_point.storage_offset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods like _transpose_ benefits from this to perform in-place operations without the need of creating a new matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 1, 2],\n",
      "        [4, 1, 7]])\n",
      " 3\n",
      " 1\n",
      " 2\n",
      " 4\n",
      " 1\n",
      " 7\n",
      "[torch.LongStorage of size 6]\n",
      "(3, 1)\n",
      "tensor([[3, 4],\n",
      "        [1, 1],\n",
      "        [2, 7]])\n",
      " 3\n",
      " 1\n",
      " 2\n",
      " 4\n",
      " 1\n",
      " 7\n",
      "[torch.LongStorage of size 6]\n",
      "(1, 3)\n",
      "tensor([[3, 4],\n",
      "        [1, 1],\n",
      "        [2, 7]])\n",
      " 3\n",
      " 1\n",
      " 2\n",
      " 4\n",
      " 1\n",
      " 7\n",
      "[torch.LongStorage of size 6]\n",
      "(1, 3)\n"
     ]
    }
   ],
   "source": [
    "m = torch.tensor([[3, 1, 2], [4, 1, 7]])\n",
    "print(m)\n",
    "print(m.storage())\n",
    "print(m.stride())\n",
    "\n",
    "m_t = m.t()\n",
    "print(m_t)\n",
    "print(m_t.storage())\n",
    "print(m_t.stride())\n",
    "\n",
    "m_t2 = m.transpose(0,1)\n",
    "print(m_t2)\n",
    "print(m_t2.storage())\n",
    "print(m_t2.stride())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some tensor operations in PyTorch only work on contiguous tensors (such as _view_), i.e. a tensor whose values are laid out in the storage from the rightmost dimension onward (for example, in a 2D tensor, the values of each file are contiguous). In such tensors, we can visit each element efficiently without jumping around in the storage.\n",
    "\n",
    "In the previous example, our initial tensor was contiguous and its transpose was not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(m.is_contiguous())\n",
    "print(m_t.is_contiguous())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a contiguous tensor from one that is not, but the storage will be reshuffled and the stride will change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 4],\n",
      "        [1, 1],\n",
      "        [2, 7]])\n",
      " 3\n",
      " 1\n",
      " 2\n",
      " 4\n",
      " 1\n",
      " 7\n",
      "[torch.LongStorage of size 6]\n",
      "False\n",
      "tensor([[3, 4],\n",
      "        [1, 1],\n",
      "        [2, 7]])\n",
      "(2, 1)\n",
      " 3\n",
      " 4\n",
      " 1\n",
      " 1\n",
      " 2\n",
      " 7\n",
      "[torch.LongStorage of size 6]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(m_t)\n",
    "print(m_t.storage())\n",
    "print(m_t.is_contiguous())\n",
    "\n",
    "m_t_cont = m_t.contiguous()\n",
    "print(m_t_cont)\n",
    "print(m_t_cont.stride())\n",
    "print(m_t_cont.storage())\n",
    "print(m_t_cont.is_contiguous())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-place operations\n",
    "The _Tensor_ object has a small number of _in place_ operations. They are recognized from a trailing underscore in their name.\n",
    "\n",
    "Any method without the trainling underscore levaes the source tensor unchanged and instead returns a new tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(3, 2)\n",
    "a.zero_()\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving tensors to GPU\n",
    "Currently, pyTorch tensor operations can be improved with the use of CUDA based GPU's (NVIDIA), ROCm based GPU's (AMD) and Google TPU's.\n",
    "\n",
    "In addition to _dtype_, a PyTorch _Tensor_ has the notion of _device_, which is where on the computer the tensor data is placed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 1.],\n",
       "        [5., 3.],\n",
       "        [2., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_gpu = torch.tensor([[4., 1.], [5., 3.], [2., 1.]], device='cuda')\n",
    "points_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could instead copy a tensor created on the CPU onto the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 1.],\n",
       "        [5., 3.],\n",
       "        [2., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4., 1.], [5., 3.], [2., 1.]])\n",
    "points_gpu = points.to(device='cuda')\n",
    "points_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case we have several GPU, we can decide on which GPU we allocate the tensor by passing an index indentifying the correspondant GPU (for example: 'cuda:**0**').\n",
    "\n",
    "Once we move a tensor onto the GPU, all operations performed on it will be executed by the GPU and the resulting tensors allocated on it. Then, is possible to move our tensors back to the CPU. Instead of the _to_ method, we can use the methods _cuda_ and _cpu_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 1.],\n",
      "        [5., 3.],\n",
      "        [2., 1.]], device='cuda:0')\n",
      "tensor([[4., 1.],\n",
      "        [5., 3.],\n",
      "        [2., 1.]])\n"
     ]
    }
   ],
   "source": [
    "points = torch.tensor([[4., 1.], [5., 3.], [2., 1.]])\n",
    "points_gpu = points.cuda() # points.cuda(0)\n",
    "print(points_gpu)\n",
    "points_cpu = points_gpu.cpu()\n",
    "print(points_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy interoperatibility\n",
    "PyTorch tensors can be converted to NumPy arrays and viceversa very efficiently. It is important to note that, if both objects are onto CPU, then both will share the same data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n",
      "<class 'numpy.ndarray'> float32\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "<class 'torch.Tensor'> torch.float32\n",
      "tensor([[ 1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1., 99.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "points = torch.ones(3, 4)\n",
    "points_np = points.numpy()\n",
    "\n",
    "print(points_np)\n",
    "print(type(points_np), points_np.dtype)\n",
    "\n",
    "points2 = torch.from_numpy(points_np)\n",
    "print(points2)\n",
    "print(type(points2), points2.dtype)\n",
    "\n",
    "points_np[1][2] = 99\n",
    "print(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serializing tensors\n",
    "PyTorch uses **pickle** under the hood to serialized the tensor object, plus dedicated serialization code for the storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = torch.tensor([[4., 1.], [5., 3.], [2., 1.]])\n",
    "torch.save(points, 'data/points.t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative, we can use a file descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/points.t', 'wb') as f:\n",
    "    torch.save(points, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading our points back is similarly a one-liner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 1.],\n",
       "        [5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.load('data/points.t')\n",
    "points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, equivalently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 1.],\n",
       "        [5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/points.t', 'rb') as f:\n",
    "    points = torch.load(f)\n",
    "points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serializing to HDF5\n",
    "In cases where you need interoperatibility with other libraries, you can use the **HDF5** format and library ([www.hdfgroup.org/solutions/hdf5](https://www.hdfgroup.org/solutions/hdf5)). HDF5 is a portable, widely supported format for representing serialized multidimensional arrays, organized in a nested key-value dictionary.\n",
    "\n",
    "Python supports HDF5 through the _hdf5_ library ([www.h5py.org](https://www.h5py.org)) which accepts and returns data in the form of NumPy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "points = torch.tensor([[4., 1.], [5., 3.], [2., 1.]])\n",
    "\n",
    "f = h5py.File('data/points.hdf5', 'w')\n",
    "dset = f.create_dataset('coords', data=points.numpy())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here _coords_ is a **key** into the HDF5 file, associated with the saved data. We can use that key to access the required data in the file, even a indexed slice of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 1.]]\n",
      "tensor([[2., 1.]])\n"
     ]
    }
   ],
   "source": [
    "f = h5py.File('data/points.hdf5', 'r')\n",
    "dset = f['coords']\n",
    "last_points = dset[-1:]\n",
    "print(last_points)\n",
    "torch_points = torch.from_numpy(dset[-1:])\n",
    "print(torch_points)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is not loaded when we open the file. The data stays on disk until we request the data (the last row of the matrix in our example). At that point, _h5py_ will return a NumPy array"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
