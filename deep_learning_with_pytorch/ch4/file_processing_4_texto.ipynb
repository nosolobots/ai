{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesado de Ficheros IV\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Representando texto\n",
    "---\n",
    "Al igual que otros, un campo de la AI donde el _deep learning_ está teniendo un fuerte impacto es el del **_natural_language_processing_** (NLP), especialmente mediante el empleo de **_recurrent neural netoworks_** (RNNs), que consumen una combinación de nuevas entradas y la salida de modelos previos. Más recientemente, una nueva clase de redes denominadas _transformers_ con modos más flexibles de incorporar información pasada ha tenido un gran impacto. Actualmente, en el area del NLP, en lugar de diseñar sofisticados _pipelines_ de varias etapas y que incluían reglas gramaticales codificadas, se diseñan redes que se entrenan sobre un _corpus_ dejando que esas reglas \"emerjan\" de los datos.\n",
    "\n",
    "En este caso, nuestro objetivo será transformar un texto en algo que pueda procesar una red neuronal, un tensor de números.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Convirtiendo texto a números\n",
    "---\n",
    "Hay dos niveles en los que una red puede trabajar con un texto, a nivel de carácter o a nivel de palabra. En todo caso, elijamos la opción que elijamos, aplicaremos codificación **on-hot** sobre la entidad correspondiente.\n",
    "\n",
    "Vamos a empezar con codificación a nivel de caracter.\n",
    "\n",
    "Primeramente, deberemos hacernos con cierta cantidad de texto. Tenemos múltiples opciones: [Proyecto Gutenberg](http://www.gutenberg.org), el _corpus_ de Wikipedia (1.9 miles de millones de palabras, 4.4 millones de artículos), [English Corpora](http://www.english-corpora.org),...\n",
    "\n",
    "Para nuestra práctica vamos a utilizar el libro _Pride and Prejudice_ de Jane Austen descargado de la web del Proyecto Gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeff\\nThe Project Gutenberg EBook of Pride and Prejudice, by Jane Austen\\n\\nThis eBook is for the use of a'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"data/text/1342-0.txt\", encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.1 Codificación one-hot de caracteres\n",
    "---\n",
    "Previamente al procesado del texto, deberemos hacer varias asunciones y preparaciones, con objeto de facilitar las tareas posteriores. Una sería limitar la codificación texto-binario que vamos a procesar (por ejemplo, ASCII). Otra, por ejemplo, sería convertir todo el texto a minúsculas, eliminar puntuaciones, números,...\n",
    "\n",
    "El siguiente paso sería recorrer todo el texto y proporcionar una codificación _on-hot_ para cada carácter, consistente en un vector de longitud el _corpus_ de caracteres con todos los valores a 0 salvo para el índice del carácter que estamos codificacndo, que valdrá 1.\n",
    "\n",
    "A modo de ejemplo, vamos a extraer una línea del texto para hacer nuestras pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'      “Impossible, Mr. Bennet, impossible, when I am not acquainted'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = text.split('\\n')\n",
    "line = lines[339]\n",
    "line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, vamos a crear un tensor que pueda almacenar todos los caracteres (codificados one-hot) para la línea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([67, 128])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "letter_t = torch.zeros(len(line), 128)  # ASCII limit\n",
    "letter_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro tensor tendrá una fila por cada carácter de la frase, y 128 columnas correspondientes a la codificación _one-hot_ del mismo. El número de columnas viene determinado por el conjunto de los diferentes caracteres del _corpus_ del texto (en nuestro caso, sólo ASCII).\n",
    "\n",
    "Ahora, lo único que tendremos que hacer, será, para cada caracter del texto, colocar un 1 en el índice que le corresponda (según su codificación ASCII)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, letter in enumerate(line.lower().strip()):\n",
    "    letter_index = ord(letter) if ord(letter) < 128 else 0\n",
    "    letter_t[i, letter_index] = 1\n",
    "letter_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.2 Codificación one-hot de palabras\n",
    "---\n",
    "La codificación _one-hot_ de palabras se puede hacer de la misma manera que de caracteres, estableciendo un vocabulario y la codificación correspondiente de las palabras del texto. Debido a que el tamaño de un vocabulario es extramadamente amplio, este esquema no es muy práctico. Un modelo más eficiente se basa, como veremos posteriormente, en el empleo de _embeddings_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a procesar la linea anterior y codificarla por palabras. Primeramente, preprocesaremos la frase para elminar signos de puntuación y convertir a minúsculas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('      “Impossible, Mr. Bennet, impossible, when I am not acquainted',\n",
       " ['impossible',\n",
       "  'mr',\n",
       "  'bennet',\n",
       "  'impossible',\n",
       "  'when',\n",
       "  'i',\n",
       "  'am',\n",
       "  'not',\n",
       "  'acquainted'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_words(input_str):\n",
    "    punctuation = '.,;:\"!?“”_-'\n",
    "    word_list = input_str.lower().replace('\\n', \" \").split()\n",
    "    word_list = [word.strip(punctuation) for word in word_list]\n",
    "    return word_list\n",
    "\n",
    "words_in_line = clean_words(line)\n",
    "line, words_in_line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear ahora un _mapping_ de palabras a índices para nuestro encoding (el vocabulario). Básicamente, construiremos un diccionario a partir del texto, usando cada palabra como clave y un índice como valor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7278, 3383)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = sorted(set(clean_words(text)))\n",
    "word2index_dict = {word:i for (i,word) in enumerate(word_list)}\n",
    "\n",
    "len(word2index_dict), word2index_dict['impossible']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver cómo resultaría la codificcación _one-hot_ de nuestra línea de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 3383 impossible\n",
      " 1 4298 mr\n",
      " 2  796 bennet\n",
      " 3 3383 impossible\n",
      " 4 7071 when\n",
      " 5 3304 i\n",
      " 6  397 am\n",
      " 7 4425 not\n",
      " 8  220 acquainted\n",
      "torch.Size([9, 7278])\n"
     ]
    }
   ],
   "source": [
    "word_t = torch.zeros(len(words_in_line), len(word2index_dict))\n",
    "for i,word in enumerate(words_in_line):\n",
    "    word_index = word2index_dict[word]\n",
    "    word_t[i, word_index] = 1\n",
    "    print(f\"{i:2} {word_index:4} {word}\")\n",
    "\n",
    "print(word_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La elección entre codificación basada en caracteres o palabras supone asumir ciertas consideraciones. Por un lado, existen muchos menos caracteres que palabras, por lo que necesitamos muchas menos clases para representarlos. Por otro, las palabras contienen mucho más significado que los caracteres individuales, por los que codificar palabras es mucho más informativo. En la práctica, se suelen adoptar caminos intermedios, como el _byte pairs encoding method_: se empieza con un diccionario de letras individuales para ir añadiendo los grupos de caracteres que aparecen con mayor frecuencia hasta alcanzar el tamaño de diccionario predefinido.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](data/image-lect/encoding-words.png)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 _Text embeddings_\n",
    "---\n",
    "La codificación _one-hot_ es muy úyil para representar datos nominales o categóricos, pero empieza a fallar cuando el número de items a codificar es excesivamente grande, como las palabras de un _corpus_ lingüístico. En el ejemplo anterior, para un único libro, tenemos sobre 7000 items!. Una codificación _one-hot_ de tales dimensiones es claramente excesiva.\n",
    "\n",
    "Podemos hacer cierto preprocesado eliminando duplicados, condensado tiempos verbales,... pero sería un trabajo ímprobo. Además, nuevas palabras supondrían añadirlas al encoding y entrenar de nuevo.\n",
    "\n",
    "El **_embedding_** consiste en mapear las palabras en un vector de números floatantes de tamaño predeterminado (por ejemplo, 100). En principio, podríamos iterar sobre nuestro vocabulario y generar 100 números aleatorios para el vector correspondiente, pero omitiría cualquier tipo de distancia basada en significado o contenido entre las palabras. Lo ideal sería que palabras empleadas en contextos similares fueran mapeadas a regiones próximas del _embedding_.\n",
    "\n",
    "Por ejemplo, si decidiéramos construir manualmente nuestro _embedding_, podríamos simplemente empezar con un mapeo básico a nombres y adjetivos (dos ejes). Así, podríamos distribuir las distintas palabras sobre este espacio 2D como muestra la siguiente figura:\n",
    "\n",
    "<br>\n",
    "\n",
    "![](data/image-lect/embedding.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "Podemos ver como palabras como las palabras que referencian colores se alinean sobre el eje de los adjetivos mientras que otras, como _dog_, _fruit_ o _flower_, sobre el eje de los nombres. En torno a ellas, situaríamos variantes de perros, frutas o flores más próximas a aquellos nombres o adjetivos relacionados.\n",
    "\n",
    "Sin duda, una clasificación manual de este tipo sería imprácticable para un _corpus_ grande y con múltiples ejes. Sin embargo, _embeddings_ (de entre 100 y 1000 dimensiones) pueden construirse de forma automática mediante el empleo de redes neuronales que se encargarán de crear _clusters_ de palabras a partir del contexto y proximidad a otras palabras en el texto.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
